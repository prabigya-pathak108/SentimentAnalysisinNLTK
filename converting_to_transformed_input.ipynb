{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import enchant   #for spelling correction and checking\n",
    "from nltk.metrics import edit_distance  # to find the case where spelling correction is needed\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet as wn,stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Lets define some function for replacement of common sentece use cases\n",
    "replacement_patterns = [\n",
    " (r'won\\'t', 'will not'),\n",
    " (r'can\\'t', 'cannot'),\n",
    " (r'i\\'m', 'i am'),\n",
    " (r'', ''),\n",
    " (r'wanna', 'want'),\n",
    " (r'gonna', 'going to'),\n",
    " (r'ain\\'t', 'is not'),\n",
    " (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    " (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    " (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    " (r'(\\w+)\\'s', '\\g<1> is'),\n",
    " (r'(\\w+)\\'re', '\\g<1> are'),\n",
    " (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n",
    "\n",
    "def replace_function(text):\n",
    "    s = text\n",
    "    for (pattern, repl) in patterns:\n",
    "        s = re.sub(pattern, repl, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#the following function remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stopwords_list=stopwords.words(\"english\")\n",
    "    text_without_stopword=\"\"\n",
    "    for i in str(text).split():\n",
    "        if i not in stopwords_list:\n",
    "            text_without_stopword=text_without_stopword+\" \"+str(i).lower()\n",
    "    return text_without_stopword.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#the following function is used for spelling checking and correction\n",
    "def correct_spellings_all(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if d.check(word):\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            suggestions = d.suggest(word)\n",
    "            if suggestions:\n",
    "                if (edit_distance(word,suggestions[0])>1):\n",
    "                    corrected_words.append(suggestions[0])\n",
    "                else:\n",
    "                    corrected_words.append(word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#the follwing function is used for lammetizing by finding the POS\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_sentence(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    corrected_words = []\n",
    "    for token, tag in pos_tag(words):\n",
    "        lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "        corrected_words.append(lemma)\n",
    "    return ' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Apply preprocessing steps to the text\n",
    "    processed_text = replace_function(text)\n",
    "    processed_text = remove_stopwords(processed_text)\n",
    "    processed_text = correct_spellings_all(processed_text)\n",
    "    processed_text = lemmatize_sentence(processed_text)\n",
    "    return processed_text\n",
    "\n",
    "def expression_check(prediction_input):\n",
    "    if prediction_input == 0:\n",
    "        return \"It has Negative Sentiment.\"\n",
    "    elif prediction_input == 1:\n",
    "        return \"It has Positive Sentiment.\"\n",
    "    else:\n",
    "        return \"Invalid Statement.\"\n",
    "\n",
    "def predict_from_user_input(user_input, model, cv):\n",
    "    # Preprocess the user input\n",
    "    processed_input = preprocess_text(user_input)\n",
    "    \n",
    "    # Transform the preprocessed input into numerical features\n",
    "    input_data = cv.transform([processed_input])\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    predicted_class = model.predict(input_data)\n",
    "\n",
    "    predicted_probabilities = model.predict_proba(input_data)\n",
    "    predicted_class_index = np.argmax(predicted_probabilities)\n",
    "    probability_predicted = model.predict_proba(input_data)\n",
    "    confidence = predicted_probabilities[0, predicted_class_index]\n",
    "    print(probability_predicted,confidence)\n",
    "    \n",
    "    prediction_msg =expression_check(predicted_class)\n",
    "\n",
    "    return prediction_msg\n",
    "\n",
    "# Load the saved model\n",
    "with open('sentiment_analysis_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load the CountVectorizer\n",
    "with open('cv.pkl', 'rb') as f:\n",
    "    cv = pickle.load(f)\n",
    "\n",
    "## answer is 112669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41742857 0.58257143]] 0.5825714285714284\n",
      "It has Positive Sentiment.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I am happy today and I want to dance.\"\n",
    "result = predict_from_user_input(user_input, model, cv)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "112669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
